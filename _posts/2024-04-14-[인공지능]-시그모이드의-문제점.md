---
layout: post
title: "[인공지능] 시그모이드의 문제점"
subtitle: 0을 중심(zero-centered)으로 하지 않는 함수의 문제점
categories: ["인공지능"]
tags: [인공지능, 활성화함수]
---

### 들어가며

인공지능을 배우면서 처음 만나는 활성화 함수는 **시그모이드**입니다.  
수식으로는 $\sigma(x)=\frac{1}{1+e^{-x}}$로 표현하며, 시각적으로는 아래와 같은 형태를 띄게 됩니다.

![sigmoid function](/assets/images/sigmoid.png){: width="50%", height="50%"}

하지만 시그모이드에는 **3가지 문제점**이 있어 잘 사용되지 않습니다.

### 1. 기울기소멸 문제

위의 그림을 보시면 (-5, 5)범위를 벗어나는 숫자의 미분값(기울기)는 0에 수렴함을 볼 수 있습니다. 딥러닝에서 기울기가 0이되어 뉴련의 **가중치(weight)도 0이 되면 그 부분은 더 이상 학습이 이루어지지 않기 때문**에 바람직하지 않습니다.

### 2. 0을 중심으로 하지 않는 함수

이 글을 쓰게된 주된 이유입니다. **"0을 중심으로 한다"** 라는게 어떤 의미인지 부터 하겠습니다. 함수의 평균이라고 생각하셔도 되고, 그림상에서의 x가 0인 지점 즉 중간부분이라고 생각하셔도 됩니다. 그림을 보면 시그모이드의 중심은 0.5 즉 0이 아닙니다.

**그럼 이게 왜 문제가 되는가?**

다시 한번 그림을 보면 시그모이드의 y축에는 음수가 없습니다. **모든 출력값이 양수**라는 의미이죠. 시그모이드의 미분은 $\sigma(1-\sigma)$으로 시그모이드의 값이 0~1이기에 **항상 미분값이 양수**가 됩니다.

![sigzag](/assets/images/sigmoid%20zigzag.png){: width="40%", height="40%"}

<center>cs-231n강의에서 발췌</center><br/>

시그모이드의 미분의 부호가 고정적이므로 **모든 가중치가 같은 방향(+,-)으로 업데이트**되어 위 그림처럼 지그재그 형태로 이루어지기 때문에 비효율적입니다.

> $\frac{\partial Loss}{\partial w}=\frac{\partial Loss}{\partial \sigma}\frac{\partial \sigma}{\partial w}$에서 $\frac{\partial \sigma}{\partial w}$의 값이 항상 양수이므로 $\frac{\partial Loss}{\partial \sigma}$의 부호에 따라 업데이트됩니다.

### 3. 많은 연산량

자주 비교되는 ReLU는 $max(0,x)$의 형태를 띈 함수이므로 비교적 연산량이 매우 많습니다. 미분값도 음수면 0, 양수면 1로 매우 편하죠.
